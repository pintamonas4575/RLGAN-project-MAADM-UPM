{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from collections import deque\n",
    "\n",
    "env = gym.make('LunarLander-v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(tf.keras.Model): \n",
    "    \"\"\"Perceptron multicapa de 2 capas de 32 y una se salida\"\"\" \n",
    "    def __init__(self, num_actions: int): \n",
    "        super(DQN, self).__init__() \n",
    "        self.dense1 = keras.layers.Dense(32, activation=\"relu\") \n",
    "        self.dense2 = keras.layers.Dense(32, activation=\"relu\") \n",
    "        self.dense3 = keras.layers.Dense(num_actions, dtype=tf.float32) # No activation \n",
    "\n",
    "    def call(self, x): \n",
    "        \"\"\"Construcción de las capas\"\"\" \n",
    "        x = self.dense1(x) \n",
    "        x = self.dense2(x) \n",
    "        return self.dense3(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object): \n",
    "    \"\"\"Experience replay buffer that samples uniformly.\"\"\" \n",
    "    def __init__(self, size): \n",
    "        self.buffer = deque(maxlen=size) \n",
    "\n",
    "    def add(self, state, action, reward, next_state, done): \n",
    "        self.buffer.append((state, action, reward, next_state, done)) \n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.buffer) \n",
    "    \n",
    "    def sample(self, num_samples): \n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], [] \n",
    "        idx = np.random.choice(len(self.buffer), num_samples) \n",
    "        for i in idx: \n",
    "            elem = self.buffer[i] \n",
    "            state, action, reward, next_state, done = elem \n",
    "            states.append(np.array(state, copy=False)) \n",
    "            # actions.append(np.array(action, copy=False)) \n",
    "            actions.append(np.array(action)) \n",
    "            rewards.append(reward) \n",
    "            next_states.append(np.array(next_state, copy=False)) \n",
    "            dones.append(done) \n",
    "        states = np.array(states) \n",
    "        actions = np.array(actions) \n",
    "        rewards = np.array(rewards, dtype=np.float32) \n",
    "        next_states = np.array(next_states) \n",
    "        dones = np.array(dones, dtype=np.float32) \n",
    "        return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Entrenamiento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparámetros \n",
    "epsilon = 1.0 \n",
    "batch_size = 128 \n",
    "discount = 0.99 \n",
    "learning_rate = 0.0001\n",
    "cur_frame = 0 \n",
    "num_actions = int(env.action_space.n)\n",
    "\n",
    "buffer = ReplayBuffer(100000) \n",
    "main_nn = DQN(num_actions) # Red principal \n",
    "target_nn = DQN(num_actions) # Red objetivo \n",
    "mse = keras.losses.MeanSquaredError()\n",
    "optimizer = keras.optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_epsilon_greedy_action(state, epsilon): \n",
    "    \"\"\"Acción aleatoria con probabilidad menor que epsilon, en otro caso la mejor.\"\"\" \n",
    "    result = tf.random.uniform((1,)) \n",
    "    if result < epsilon: \n",
    "        return env.action_space.sample() # Elegimos una acción aleatória \n",
    "    else: \n",
    "        return tf.argmax(main_nn(state)[0]).numpy() # Elección de acción Greedy. \n",
    "    \n",
    "@tf.function # Configuración de cada iteración de entrenamiento \n",
    "def train_step(states, actions, rewards, next_states, dones): \n",
    "    # Calculo de los objetivos (segunda red) \n",
    "    next_qs = target_nn(next_states) \n",
    "    max_next_qs = tf.reduce_max(next_qs, axis=-1) \n",
    "    target = rewards + (1. - dones) * discount * max_next_qs \n",
    "    with tf.GradientTape() as tape: \n",
    "        qs = main_nn(states) \n",
    "        action_masks = tf.one_hot(actions, num_actions) \n",
    "        masked_qs = tf.reduce_sum(action_masks * qs, axis=-1) \n",
    "        loss = mse(target, masked_qs) \n",
    "    grads = tape.gradient(loss, main_nn.trainable_variables) \n",
    "    optimizer.apply_gradients(zip(grads, main_nn.trainable_variables)) \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1000\n",
    "trace = 50\n",
    "for episode in range(num_episodes+1): \n",
    "    state, _ = env.reset()\n",
    "    ep_reward, done = 0, False\n",
    "    while not done or truncated: \n",
    "        state_in = tf.expand_dims(state, axis=0) \n",
    "        action = select_epsilon_greedy_action(state_in, epsilon) \n",
    "        next_state, reward, done, truncated, _ = env.step(action) \n",
    "        ep_reward += reward \n",
    "        buffer.add(state, action, reward, next_state, done) \n",
    "        state = next_state \n",
    "        cur_frame += 1 \n",
    "        # Copiamos los pesos de main_nn a target_nn. \n",
    "        if cur_frame % 2000 == 0: \n",
    "            target_nn.set_weights(main_nn.get_weights()) \n",
    "        # ---------------------------------------------\n",
    "        if len(buffer) >= batch_size: \n",
    "            states, actions, rewards, next_states, dones = buffer.sample(batch_size) \n",
    "            loss = train_step(states, actions, rewards, next_states, dones) \n",
    "    if episode < 950: \n",
    "        epsilon -= 0.001 \n",
    "    if episode % trace == 0:\n",
    "        print(f'Episode {episode}/{num_episodes}. Epsilon: {epsilon:.3f}. Reward: {ep_reward:.3f}')\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
